{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.script import *\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_docs(\n",
    "    dir_comments: str, # directory containing parquet dataframes\n",
    "    subreddit: str # subreddit to be processed\n",
    "    ) -> list:\n",
    "    dir_comments = Path(dir_comments)\n",
    "    df = pl.read_parquet(dir_comments / subreddit / '*.parquet')\n",
    "    print('finished reading parquet files')\n",
    "    df = df.sample(10_000)\n",
    "    docs_srs = df['body'].str.split(' ')\n",
    "    print('finished tokenizing')\n",
    "    docs_list = docs_srs.to_list()\n",
    "    print('finished converting to list')\n",
    "    # return docs_list\n",
    "    return docs_srs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished reading parquet files\n",
      "finished tokenizing\n",
      "finished converting to list\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "docs = get_docs('/Users/quirin/proj/getreddit/out/', 'Coronavirus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpaths = list(Path('/Users/quirin/proj/getreddit/out/Coronavirus').glob('*.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(fpaths[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['body'].str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                     [Yes]\n",
       "1         [Just, eat, raw, elk, meat, and, work, out, su...\n",
       "2         [They, had, the, Big, Pendleton, concert, whic...\n",
       "3         [Agreed,, and, I, don’t, understand, this, at,...\n",
       "4         [Some?, \\n\\n#SOME, vaccinated, Americans?, \\n\\...\n",
       "                                ...                        \n",
       "508550    [They, need, more, testing., Seems, like, them...\n",
       "508551                                      [Oh, my, gosh!]\n",
       "508552    [Among, documented, cases., We're, still, mass...\n",
       "508553                                          [[removed]]\n",
       "508554                                          [[removed]]\n",
       "Name: body, Length: 508555, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    docs[:10_000],\n",
    "    workers=8,\n",
    "    callbacks=[my_callback],\n",
    "    min_count=5,\n",
    "    window=5,\n",
    "    epochs=5,\n",
    "    vector_size=300,\n",
    "    batch_words=10_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4712"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get number of words in model\n",
    "len(model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('know', 0.9908594489097595),\n",
       " ('understand', 0.9546481966972351),\n",
       " ('say', 0.9420730471611023),\n",
       " ('feel', 0.9382495284080505),\n",
       " ('care', 0.9364539980888367),\n",
       " ('don’t', 0.9326096177101135),\n",
       " (\"don't\", 0.9324885010719299),\n",
       " ('mean', 0.929454505443573),\n",
       " ('like,', 0.9275861978530884),\n",
       " ('what', 0.9259113073348999)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('think')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Corpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self, docs):\n",
    "        self.docs_clean = docs\n",
    "\n",
    "    def __iter__(self):\n",
    "        for doc in self.docs_clean:\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MyCallback(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    # Initialize any variables or attributes here\n",
    "    def __init__(self):\n",
    "        self.epoch = 0 # Keep track of the current epoch number\n",
    "\n",
    "    # Do something at the start of each epoch\n",
    "    def on_epoch_begin(self, model):\n",
    "        logging.info(f\"Epoch {self.epoch} started\")\n",
    "\n",
    "    # Do something at the end of each epoch\n",
    "    def on_epoch_end(self, model):\n",
    "        logging.info(f\"Epoch {self.epoch} finished\")\n",
    "        self.epoch += 1 # Increment the epoch number\n",
    "\n",
    "    # Do something at the start of each batch\n",
    "    def on_batch_begin(self, model):\n",
    "        pass # You can add your own code here\n",
    "\n",
    "    # Do something at the end of each batch\n",
    "    def on_batch_end(self, model, cumulative_stats):\n",
    "        # Get some statistics from cumulative_stats dictionary\n",
    "        total_examples = cumulative_stats['total_examples']\n",
    "        total_words = cumulative_stats['total_words']\n",
    "        job = cumulative_stats['job']\n",
    "        raw_words = cumulative_stats['raw_words']\n",
    "        effective_words = cumulative_stats['effective_words']\n",
    "\n",
    "        # Calculate and print some percentages using these statistics\n",
    "        percentage_sentences = (job[1] - job[0]) / total_examples * 100\n",
    "        percentage_words = raw_words / total_words * 100\n",
    "        percentage_effective_words = effective_words / raw_words * 100\n",
    "\n",
    "        logging.info(f\"Batch processed {percentage_sentences:.2f}% sentences and {percentage_words:.2f}% words\")\n",
    "        logging.info(f\"Batch used {percentage_effective_words:.2f}% words effectively for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callback = MyCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@call_parse\n",
    "def train_model(\n",
    "    subreddit: str, # Subreddit to be processed\n",
    "    dir_comments: str, # Directory containing parquet dataframes\n",
    "    dir_models: str # Directory to save model\n",
    "    ) -> Word2Vec:\n",
    "    \"\"\"\n",
    "    Trains a word2vec model on the comments of a subreddit.\n",
    "    \"\"\"\n",
    "    docs = get_docs(dir_comments, subreddit)\n",
    "    corpus = Corpus(docs)\n",
    "    my_callback = MyCallback()\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    model = Word2Vec(\n",
    "        corpus,\n",
    "        workers=8,\n",
    "        callbacks=[my_callback],\n",
    "        min_count=5,\n",
    "        window=5,\n",
    "        epochs=5,\n",
    "        vector_size=300,\n",
    "        batch_words=10_000\n",
    "    )\n",
    "    dir_models = Path(dir_models)\n",
    "    model.save((dir_models / f\"{subreddit}.model\").as_posix())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "model = train_model('conspiracy', '~/proj/getreddit/out/', '~/proj/redditdumps/out/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# toy console script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@call_parse\n",
    "def add_one(\n",
    "    num: int # first number\n",
    "    ) -> int: # result\n",
    "    result = num + 1\n",
    "    print(result)\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redditdumps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "03700ea1ba2e179c7f98d62339187c382f8ecec725bd9e96d0d240b3b5825189"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
