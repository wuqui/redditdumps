{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.script import *\n",
    "from pathlib import Path\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_docs(\n",
    "\tdir_comments: str,  # directory containing parquet dataframes\n",
    "\tmax_docs=None\n",
    "\t) -> pd.Series:\n",
    "\tfpaths = list(Path(dir_comments).glob('*.parquet'))\n",
    "\tdf = pd.read_parquet(fpaths, columns=['body'])\n",
    "\tprint('finished reading parquet files')\n",
    "\tif max_docs:\n",
    "\t\tdf = df.sample(max_docs)\n",
    "\tdocs = df['body'].str.split()\n",
    "\tprint('finished tokenizing')\n",
    "\treturn docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished reading parquet files\n",
      "finished tokenizing\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "docs = get_docs('/Users/quirin/proj/getreddit/out/Coronavirus/', max_docs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Corpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self, docs):\n",
    "        self.docs_clean = docs\n",
    "\n",
    "    def __iter__(self):\n",
    "        for doc in self.docs_clean:\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Word2VecLogger(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    # Initialize any variables or attributes here\n",
    "    def __init__(self):\n",
    "        self.epoch = 0 # Keep track of the current epoch number\n",
    "\n",
    "    # Do something at the start of each epoch\n",
    "    def on_epoch_begin(self, model):\n",
    "        logging.info(f\"Epoch {self.epoch} started\")\n",
    "\n",
    "    # Do something at the end of each epoch\n",
    "    def on_epoch_end(self, model):\n",
    "        logging.info(f\"Epoch {self.epoch} finished\")\n",
    "        self.epoch += 1 # Increment the epoch number\n",
    "\n",
    "    # Do something at the start of each batch\n",
    "    def on_batch_begin(self, model):\n",
    "        pass # You can add your own code here\n",
    "\n",
    "    # Do something at the end of each batch\n",
    "    def on_batch_end(self, model, cumulative_stats):\n",
    "        # Get some statistics from cumulative_stats dictionary\n",
    "        total_examples = cumulative_stats['total_examples']\n",
    "        total_words = cumulative_stats['total_words']\n",
    "        job = cumulative_stats['job']\n",
    "        raw_words = cumulative_stats['raw_words']\n",
    "        effective_words = cumulative_stats['effective_words']\n",
    "\n",
    "        # Calculate and print some percentages using these statistics\n",
    "        percentage_sentences = (job[1] - job[0]) / total_examples * 100\n",
    "        percentage_words = raw_words / total_words * 100\n",
    "        percentage_effective_words = effective_words / raw_words * 100\n",
    "\n",
    "        logging.info(f\"Batch processed {percentage_sentences:.2f}% sentences and {percentage_words:.2f}% words\")\n",
    "        logging.info(f\"Batch used {percentage_effective_words:.2f}% words effectively for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def train_model(\n",
    "    docs,  # corpus of tokenized documents\n",
    "    min_count: int = 5,  # ignore words with frequency lower than this\n",
    "    ) -> pd.Series:\n",
    "    logger = Word2VecLogger()\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    model = Word2Vec(\n",
    "        docs,\n",
    "        workers=8,\n",
    "        min_count=min_count,\n",
    "        window=5,\n",
    "        epochs=5,\n",
    "        vector_size=300,\n",
    "        batch_words=10_000,\n",
    "        callbacks=[logger]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@call_parse\n",
    "def train_model_pipe(\n",
    "    dir_comments: str,  # Directory containing parquet dataframes\n",
    "    max_docs: int = None,  # Maximum number of parquet files to be processed\n",
    "    fp_model_out: str = None, # Save model to this file path\n",
    "    min_count: int = 5,  # ignore words with frequency lower than this\n",
    "    ) -> Word2Vec:\n",
    "    \"\"\"\n",
    "    Trains a word2vec model on the comments of a subreddit.\n",
    "    \"\"\"\n",
    "    docs = get_docs(dir_comments, max_docs)\n",
    "    corpus = Corpus(docs)\n",
    "    model = train_model(corpus, min_count=min_count)\n",
    "    if fp_model_out:\n",
    "        model.save(fp_model_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished reading parquet files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-17 11:51:06,303 : INFO : collecting all words and their counts\n",
      "2023-02-17 11:51:06,305 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2023-02-17 11:51:06,305 : INFO : collected 58 word types from a corpus of 64 raw words and 5 sentences\n",
      "2023-02-17 11:51:06,305 : INFO : Creating a fresh vocabulary\n",
      "2023-02-17 11:51:06,306 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 58 unique words (100.00% of original 58, drops 0)', 'datetime': '2023-02-17T11:51:06.306039', 'gensim': '4.3.0', 'python': '3.10.0 | packaged by conda-forge | (default, Nov 20 2021, 02:27:15) [Clang 11.1.0 ]', 'platform': 'macOS-13.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-02-17 11:51:06,306 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 64 word corpus (100.00% of original 64, drops 0)', 'datetime': '2023-02-17T11:51:06.306559', 'gensim': '4.3.0', 'python': '3.10.0 | packaged by conda-forge | (default, Nov 20 2021, 02:27:15) [Clang 11.1.0 ]', 'platform': 'macOS-13.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-02-17 11:51:06,307 : INFO : deleting the raw counts dictionary of 58 items\n",
      "2023-02-17 11:51:06,307 : INFO : sample=0.001 downsamples 58 most-common words\n",
      "2023-02-17 11:51:06,307 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 18.989318830717952 word corpus (29.7%% of prior 64)', 'datetime': '2023-02-17T11:51:06.307706', 'gensim': '4.3.0', 'python': '3.10.0 | packaged by conda-forge | (default, Nov 20 2021, 02:27:15) [Clang 11.1.0 ]', 'platform': 'macOS-13.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-02-17 11:51:06,308 : INFO : estimated required memory for 58 words and 300 dimensions: 168200 bytes\n",
      "2023-02-17 11:51:06,309 : INFO : resetting layer weights\n",
      "2023-02-17 11:51:06,309 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-02-17T11:51:06.309868', 'gensim': '4.3.0', 'python': '3.10.0 | packaged by conda-forge | (default, Nov 20 2021, 02:27:15) [Clang 11.1.0 ]', 'platform': 'macOS-13.1-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2023-02-17 11:51:06,310 : INFO : Word2Vec lifecycle event {'msg': 'training model with 8 workers on 58 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-02-17T11:51:06.310096', 'gensim': '4.3.0', 'python': '3.10.0 | packaged by conda-forge | (default, Nov 20 2021, 02:27:15) [Clang 11.1.0 ]', 'platform': 'macOS-13.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2023-02-17 11:51:06,310 : INFO : Epoch 0 started\n",
      "2023-02-17 11:51:06,312 : INFO : EPOCH 0: training on 64 raw words (24 effective words) took 0.0s, 178771 effective words/s\n",
      "2023-02-17 11:51:06,313 : INFO : Epoch 0 finished\n",
      "2023-02-17 11:51:06,313 : INFO : Epoch 1 started\n",
      "2023-02-17 11:51:06,316 : INFO : EPOCH 1: training on 64 raw words (21 effective words) took 0.0s, 49075 effective words/s\n",
      "2023-02-17 11:51:06,316 : INFO : Epoch 1 finished\n",
      "2023-02-17 11:51:06,317 : INFO : Epoch 2 started\n",
      "2023-02-17 11:51:06,319 : INFO : EPOCH 2: training on 64 raw words (19 effective words) took 0.0s, 49793 effective words/s\n",
      "2023-02-17 11:51:06,319 : INFO : Epoch 2 finished\n",
      "2023-02-17 11:51:06,319 : INFO : Epoch 3 started\n",
      "2023-02-17 11:51:06,321 : INFO : EPOCH 3: training on 64 raw words (20 effective words) took 0.0s, 60637 effective words/s\n",
      "2023-02-17 11:51:06,321 : INFO : Epoch 3 finished\n",
      "2023-02-17 11:51:06,321 : INFO : Epoch 4 started\n",
      "2023-02-17 11:51:06,324 : INFO : EPOCH 4: training on 64 raw words (24 effective words) took 0.0s, 82462 effective words/s\n",
      "2023-02-17 11:51:06,324 : INFO : Epoch 4 finished\n",
      "2023-02-17 11:51:06,324 : INFO : Word2Vec lifecycle event {'msg': 'training on 320 raw words (108 effective words) took 0.0s, 7460 effective words/s', 'datetime': '2023-02-17T11:51:06.324778', 'gensim': '4.3.0', 'python': '3.10.0 | packaged by conda-forge | (default, Nov 20 2021, 02:27:15) [Clang 11.1.0 ]', 'platform': 'macOS-13.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2023-02-17 11:51:06,325 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=58, vector_size=300, alpha=0.025>', 'datetime': '2023-02-17T11:51:06.325010', 'gensim': '4.3.0', 'python': '3.10.0 | packaged by conda-forge | (default, Nov 20 2021, 02:27:15) [Clang 11.1.0 ]', 'platform': 'macOS-13.1-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished tokenizing\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "model = train_model_pipe('/Users/quirin/proj/getreddit/out/conspiracy', max_docs=5, min_count=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
