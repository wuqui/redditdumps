{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.script import *\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_docs(\n",
    "\tdir_comments: str,  # directory containing parquet dataframes\n",
    "\t) -> pd.Series:\n",
    "\tfpaths = list(Path(dir_comments).glob('*.parquet'))\n",
    "\tdf = pd.read_parquet(fpaths, columns=['body'])\n",
    "\tprint('finished reading parquet files')\n",
    "\tdocs = df['body'].str.split()\n",
    "\tprint('finished tokenizing')\n",
    "\treturn docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs_bak(\n",
    "    dir_comments: str, # directory containing parquet dataframes\n",
    "    subreddit: str # subreddit to be processed\n",
    "    ) -> list:\n",
    "    fpaths = list(Path('/Users/quirin/proj/getreddit/out/Coronavirus').glob('*.parquet'))\n",
    "    df = pl.read_parquet(dir_comments / subreddit / '*.parquet')\n",
    "    print('finished reading parquet files')\n",
    "    df = df.sample(10_000)\n",
    "    docs_srs = df['body'].str.split(' ')\n",
    "    print('finished tokenizing')\n",
    "    docs_list = docs_srs.to_list()\n",
    "    print('finished converting to list')\n",
    "    # return docs_list\n",
    "    return docs_srs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Corpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self, docs):\n",
    "        self.docs_clean = docs\n",
    "\n",
    "    def __iter__(self):\n",
    "        for doc in self.docs_clean:\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MyCallback(gensim.models.callbacks.CallbackAny2Vec):\n",
    "    # Initialize any variables or attributes here\n",
    "    def __init__(self):\n",
    "        self.epoch = 0 # Keep track of the current epoch number\n",
    "\n",
    "    # Do something at the start of each epoch\n",
    "    def on_epoch_begin(self, model):\n",
    "        logging.info(f\"Epoch {self.epoch} started\")\n",
    "\n",
    "    # Do something at the end of each epoch\n",
    "    def on_epoch_end(self, model):\n",
    "        logging.info(f\"Epoch {self.epoch} finished\")\n",
    "        self.epoch += 1 # Increment the epoch number\n",
    "\n",
    "    # Do something at the start of each batch\n",
    "    def on_batch_begin(self, model):\n",
    "        pass # You can add your own code here\n",
    "\n",
    "    # Do something at the end of each batch\n",
    "    def on_batch_end(self, model, cumulative_stats):\n",
    "        # Get some statistics from cumulative_stats dictionary\n",
    "        total_examples = cumulative_stats['total_examples']\n",
    "        total_words = cumulative_stats['total_words']\n",
    "        job = cumulative_stats['job']\n",
    "        raw_words = cumulative_stats['raw_words']\n",
    "        effective_words = cumulative_stats['effective_words']\n",
    "\n",
    "        # Calculate and print some percentages using these statistics\n",
    "        percentage_sentences = (job[1] - job[0]) / total_examples * 100\n",
    "        percentage_words = raw_words / total_words * 100\n",
    "        percentage_effective_words = effective_words / raw_words * 100\n",
    "\n",
    "        logging.info(f\"Batch processed {percentage_sentences:.2f}% sentences and {percentage_words:.2f}% words\")\n",
    "        logging.info(f\"Batch used {percentage_effective_words:.2f}% words effectively for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callback = MyCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@call_parse\n",
    "def train_model(\n",
    "    subreddit: str, # Subreddit to be processed\n",
    "    dir_comments: str, # Directory containing parquet dataframes\n",
    "    dir_models: str # Directory to save model\n",
    "    ) -> Word2Vec:\n",
    "    \"\"\"\n",
    "    Trains a word2vec model on the comments of a subreddit.\n",
    "    \"\"\"\n",
    "    docs = get_docs(dir_comments)\n",
    "    # corpus = Corpus(docs)\n",
    "    my_callback = MyCallback()\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    model = Word2Vec(\n",
    "        docs,\n",
    "        workers=8,\n",
    "        callbacks=[my_callback],\n",
    "        min_count=5,\n",
    "        window=5,\n",
    "        epochs=5,\n",
    "        vector_size=300,\n",
    "        batch_words=10_000\n",
    "    )\n",
    "    dir_models = Path(dir_models)\n",
    "    model.save((dir_models / f\"{subreddit}.model\").as_posix())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "model = train_model('conspiracy', '/Users/quirin/proj/getreddit/out/conspiracy', '/Users/quirin/proj/redditdumps/out/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# toy console script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@call_parse\n",
    "def add_one(\n",
    "    num: int # first number\n",
    "    ) -> int: # result\n",
    "    result = num + 1\n",
    "    print(result)\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
