# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['get_docs', 'Corpus', 'MyCallback', 'train_model', 'add_one']

# %% ../nbs/00_core.ipynb 2
from fastcore.script import *
from pathlib import Path
import polars as pl
import gensim
from gensim.models import Word2Vec
import pandas as pd

# %% ../nbs/00_core.ipynb 3
def get_docs(
    dir_comments: str, # directory containing parquet dataframes
    subreddit: str # subreddit to be processed
    ) -> list:
    dir_comments = Path(dir_comments)
    df = pl.read_parquet(dir_comments / subreddit / '*.parquet')
    print('finished reading parquet files')
    df = df.sample(10_000)
    docs_srs = df['body'].str.split(' ')
    print('finished tokenizing')
    docs_list = docs_srs.to_list()
    print('finished converting to list')
    # return docs_list
    return docs_srs
    

# %% ../nbs/00_core.ipynb 13
class Corpus:
    """An iterator that yields sentences (lists of str)."""
    def __init__(self, docs):
        self.docs_clean = docs

    def __iter__(self):
        for doc in self.docs_clean:
            yield doc

# %% ../nbs/00_core.ipynb 15
class MyCallback(gensim.models.callbacks.CallbackAny2Vec):
    # Initialize any variables or attributes here
    def __init__(self):
        self.epoch = 0 # Keep track of the current epoch number

    # Do something at the start of each epoch
    def on_epoch_begin(self, model):
        logging.info(f"Epoch {self.epoch} started")

    # Do something at the end of each epoch
    def on_epoch_end(self, model):
        logging.info(f"Epoch {self.epoch} finished")
        self.epoch += 1 # Increment the epoch number

    # Do something at the start of each batch
    def on_batch_begin(self, model):
        pass # You can add your own code here

    # Do something at the end of each batch
    def on_batch_end(self, model, cumulative_stats):
        # Get some statistics from cumulative_stats dictionary
        total_examples = cumulative_stats['total_examples']
        total_words = cumulative_stats['total_words']
        job = cumulative_stats['job']
        raw_words = cumulative_stats['raw_words']
        effective_words = cumulative_stats['effective_words']

        # Calculate and print some percentages using these statistics
        percentage_sentences = (job[1] - job[0]) / total_examples * 100
        percentage_words = raw_words / total_words * 100
        percentage_effective_words = effective_words / raw_words * 100

        logging.info(f"Batch processed {percentage_sentences:.2f}% sentences and {percentage_words:.2f}% words")
        logging.info(f"Batch used {percentage_effective_words:.2f}% words effectively for training")

# %% ../nbs/00_core.ipynb 16
import logging

# %% ../nbs/00_core.ipynb 18
@call_parse
def train_model(
    subreddit: str, # Subreddit to be processed
    dir_comments: str, # Directory containing parquet dataframes
    dir_models: str # Directory to save model
    ) -> Word2Vec:
    """
    Trains a word2vec model on the comments of a subreddit.
    """
    docs = get_docs(dir_comments, subreddit)
    corpus = Corpus(docs)
    my_callback = MyCallback()
    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    model = Word2Vec(
        corpus,
        workers=8,
        callbacks=[my_callback],
        min_count=5,
        window=5,
        epochs=5,
        vector_size=300,
        batch_words=10_000
    )
    dir_models = Path(dir_models)
    model.save((dir_models / f"{subreddit}.model").as_posix())
    return model

# %% ../nbs/00_core.ipynb 21
@call_parse
def add_one(
    num: int # first number
    ) -> int: # result
    result = num + 1
    print(result)
    return result
