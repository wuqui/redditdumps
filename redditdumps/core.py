# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['get_docs', 'Corpus', 'Word2VecLogger', 'train_model', 'train_model_pipe']

# %% ../nbs/00_core.ipynb 2
from fastcore.script import *
from pathlib import Path
import gensim
from gensim.models import Word2Vec
import pandas as pd
import logging

# %% ../nbs/00_core.ipynb 3
def get_docs(
	dir_comments: str,  # directory containing parquet dataframes
	max_docs=None
	) -> pd.Series:
	fpaths = list(Path(dir_comments).glob('*.parquet'))
	df = pd.read_parquet(fpaths, columns=['body'])
	print('finished reading parquet files')
	if max_docs:
		df = df.sample(max_docs)
	docs = df['body'].str.split()
	print('finished tokenizing')
	return docs

# %% ../nbs/00_core.ipynb 6
class Corpus:
    """An iterator that yields sentences (lists of str)."""
    def __init__(self, docs):
        self.docs_clean = docs

    def __iter__(self):
        for doc in self.docs_clean:
            yield doc

# %% ../nbs/00_core.ipynb 7
class Word2VecLogger(gensim.models.callbacks.CallbackAny2Vec):
    # Initialize any variables or attributes here
    def __init__(self):
        self.epoch = 0 # Keep track of the current epoch number

    # Do something at the start of each epoch
    def on_epoch_begin(self, model):
        logging.info(f"Epoch {self.epoch} started")

    # Do something at the end of each epoch
    def on_epoch_end(self, model):
        logging.info(f"Epoch {self.epoch} finished")
        self.epoch += 1 # Increment the epoch number

    # Do something at the start of each batch
    def on_batch_begin(self, model):
        pass # You can add your own code here

    # Do something at the end of each batch
    def on_batch_end(self, model, cumulative_stats):
        # Get some statistics from cumulative_stats dictionary
        total_examples = cumulative_stats['total_examples']
        total_words = cumulative_stats['total_words']
        job = cumulative_stats['job']
        raw_words = cumulative_stats['raw_words']
        effective_words = cumulative_stats['effective_words']

        # Calculate and print some percentages using these statistics
        percentage_sentences = (job[1] - job[0]) / total_examples * 100
        percentage_words = raw_words / total_words * 100
        percentage_effective_words = effective_words / raw_words * 100

        logging.info(f"Batch processed {percentage_sentences:.2f}% sentences and {percentage_words:.2f}% words")
        logging.info(f"Batch used {percentage_effective_words:.2f}% words effectively for training")

# %% ../nbs/00_core.ipynb 8
def train_model(
    docs,  # corpus of tokenized documents
    min_count: int = 5,  # ignore words with frequency lower than this
    ) -> pd.Series:
    logger = Word2VecLogger()
    logging.basicConfig(
        format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
    model = Word2Vec(
        docs,
        workers=8,
        min_count=min_count,
        window=5,
        epochs=5,
        vector_size=300,
        batch_words=10_000,
        callbacks=[logger]
    )
    return model


# %% ../nbs/00_core.ipynb 9
@call_parse
def train_model_pipe(
    dir_comments: str,  # Directory containing parquet dataframes
    max_docs: int = None,  # Maximum number of parquet files to be processed
    fp_model_out: str = None, # Save model to this file path
    min_count: int = 5,  # ignore words with frequency lower than this
    ) -> Word2Vec:
    """
    Trains a word2vec model on the comments of a subreddit.
    """
    docs = get_docs(dir_comments, max_docs)
    corpus = Corpus(docs)
    model = train_model(corpus, min_count=min_count)
    if fp_model_out:
        model.save(fp_model_out)
    return model
